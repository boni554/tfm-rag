# Trabajo Fin de Máster - Universidad de Valladolid

## Implementación de técnicas de RAG (Retrieval Augmented Generation) sobre LLM (Large Language Models) para la extracción y generación de documentos en las Entidades Públicas   

**Realizado por Miguel Ángel Collado Alonso**

Un viejo desafío en el campo de la inteligencia artificial es “enseñar” a las máquinas a entender cómo hablan y escriben los humanos, conocido como procesamiento de lenguaje natural (PLN). Sin embargo, desde hace poco más de dos años, estamos asistiendo a la caída de este antiguo bastión con la llegada de los modelos grandes del lenguaje (LLM) y los interfaces conversacionales. 

Los LLM son modelos de inteligencia artificial que se entrenan utilizando algoritmos de Deep Learning sobre conjuntos enormes de información generada por humanos. De esta manera, una vez entrenados, han aprendido la forma en la que los humanos utilizamos la palabra hablada y escrita, así que son capaces de ofrecernos respuestas generales y con un patrón muy parecido a nuestra forma de contestar a las preguntas que les hacemos. Sin embargo, si buscamos respuestas precisas en un contexto determinado, los LLM por sí solos no proporcionarán respuestas concretas o habrá una alta probabilidad de que se inventen completamente la respuesta. 

En este trabajo, se va a explicar y desarrollar una de las técnicas clave que hace posible que estos sistemas nos respondan con relativa precisión a las preguntas que les hacemos, esta tecnología se denomina Generación Aumentada por Recuperación o RAG, del inglés Retrieval Augmented Generation. 
